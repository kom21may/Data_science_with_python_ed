{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc362a1",
   "metadata": {},
   "source": [
    "We shall use the same dataset used in the previous assignment -digits.\n",
    "\n",
    "Make an80-20 train/test split.[Hint:Explore datasets module from scikit learn]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ca7746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc18fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digit=load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9936c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digit.data, digit.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467313b5",
   "metadata": {},
   "source": [
    "\n",
    "2.Using scikit learn to perform an LDA on the dataset. Find out the number of components in the projected subspace.[Hint:Refer to a discriminant analysis moduleof scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9caa2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda=LinearDiscriminantAnalysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e95f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lda=lda.fit_transform(X_train,y_train)\n",
    "X_test_lda=lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd40f437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of effective components in the projected subspace: 9\n"
     ]
    }
   ],
   "source": [
    "# Get the effective number of components in the projected subspace\n",
    "num_components = min(len(set(digit.target)) - 1, digit.data.shape[1])\n",
    "\n",
    "print(f\"Number of effective components in the projected subspace: {num_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5205b",
   "metadata": {},
   "source": [
    "In the context of the load_digits() dataset, there are 10 classes (digits 0 to 9), and each data point consists of 64 features (8x8 pixel images). Therefore, the maximum number of components that can be used in LDA for this dataset is min(10 - 1, 64), which is 9.\n",
    "\n",
    "This means that even if you apply LDA to the dataset, you will be limited to using at most 9 components in the projected subspace. LDA will find a transformation that best separates the classes using these components.\n",
    "\n",
    "Regarding the effective number of components, it's not that the components are \"degraded\" without the use of LDA. LDA focuses on finding the most discriminative directions for class separation. If you don't explicitly use LDA, you might use all available features (components), but they might not be optimized for class separation.\n",
    "\n",
    "To clarify, in the context of LDA, the number of components is predefined based on the dataset's characteristics (number of classes and features), and LDA's goal is to find the optimal projection into this reduced-dimensional space. In contrast, techniques like PCA allow you to choose the number of components based on the amount of variance you want to retain, which can be more flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc86ec15",
   "metadata": {},
   "source": [
    "3.Transform the dataset and fit a logistic regression and observe the accuracy. Compare it with the previous model based on PCA in terms of accuracy and model complexity.[Hint: Project both the train and test samples to the new subspace]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10c5d795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lr=LogisticRegression(max_iter=1000)\n",
    "Lr.fit(X_lda,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d70a7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2=Lr.predict(X_test_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66f0db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0127a385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9472222222222222"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c13d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "032992aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform PCA\n",
    "pca = PCA(0.95)  # Retain 95% of the variance\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model on the PCA-transformed data\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred1 = model.predict(X_test_pca)\n",
    "accuracy_score(y_test,y_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e78043",
   "metadata": {},
   "source": [
    "PCA focuses on capturing overall variance in the data and is not directly guided by class separability, while LDA aims to maximize class separability. In this particular case, it appears that the overall variance captured by PCA might be contributing to its slightly better performance on accuracy.\n",
    "\n",
    "However, if you have specific goals, such as prioritizing class separation or reducing model complexity, LDA might still be a valuable choice, even if it results in slightly lower accuracy in this scenario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
